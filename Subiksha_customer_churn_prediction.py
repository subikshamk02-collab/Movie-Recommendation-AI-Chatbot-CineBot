# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction_Mindineous.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rruenzJuP_IHq7BchSAYusL1SGHvNcBS
"""

# ============================================
# Flexible Data Import for Churn Project
# ============================================
# Option 1: Load CSV directly from Google Drive
# Option 2: Manually upload CSV from your computer
# ============================================

import pandas as pd
import os

# Change this path to match where your file is stored in Drive
drive_path = "/content/drive/MyDrive/Mindenious_Subiksha_Churn_Prediction/churn_data.csv"

# Check if the file exists in Google Drive
if os.path.exists(drive_path):
    print("CSV found in Google Drive. Loading from Drive...")
    df = pd.read_csv(drive_path)

else:
    # If not found, ask user to upload manually
    print("CSV not found in Drive. Please upload the file manually below.")
    from google.colab import files
    uploaded = files.upload()

    # Take the first uploaded file automatically
    filename = list(uploaded.keys())[0]
    df = pd.read_csv(filename)

# Display confirmation and dataset preview
print("Dataset loaded successfully. Shape:", df.shape)
df.head()

import pandas as pd

# Replace the filename below if it differs in your extracted folder
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

# Step 8: Display the first few rows of the dataset to verify loading
df.head()

# ============================================================
# Stage 1: Import Libraries and Initial Setup
# ============================================================

# Step 1: Import common data analysis libraries
import numpy as np                 # For numerical operations
import pandas as pd                # For handling and analyzing data
import matplotlib.pyplot as plt    # For creating plots and visualizations
import seaborn as sns              # For better-looking statistical plots

# Step 2: Import machine learning tools from scikit-learn
from sklearn.model_selection import train_test_split   # For splitting data into train/test sets
from sklearn.preprocessing import LabelEncoder, StandardScaler  # For encoding and scaling
from sklearn.linear_model import LogisticRegression     # Logistic Regression model
from sklearn.ensemble import RandomForestClassifier     # Random Forest model
from sklearn.metrics import (accuracy_score, confusion_matrix,
                             classification_report, roc_auc_score)  # Model evaluation metrics

# Step 3: Set display options and visualization styles
pd.set_option('display.max_columns', None)  # Show all columns in outputs
sns.set(style="whitegrid")                  # Use Seaborn’s clean white grid theme

# Step 4: Check basic dataset information
df.info()

# Step 5: View statistical summary of numerical columns
df.describe()

# Stage 2: Data Cleaning

# Display basic dataset information
df.info()
df.describe()

# Check for missing values
print(df.isnull().sum())

# Replace spaces in 'TotalCharges' with NaN and convert to numeric
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan)
df['TotalCharges'] = df['TotalCharges'].astype(float)

# Fill missing 'TotalCharges' with median value
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Drop 'customerID' since it’s just an identifier
df.drop('customerID', axis=1, inplace=True)

# Convert target variable 'Churn' to numeric (Yes → 1, No → 0)
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Encode categorical variables using LabelEncoder
cat_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Display first few rows after cleaning
df.head()

# Stage 3: EDA (Exploratory Data Analysis)

# Check churn rate
plt.figure(figsize=(5, 4))
sns.countplot(x='Churn', data=df)
plt.title("Churn Distribution")
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

# Tenure vs Churn
plt.figure(figsize=(6, 4))
sns.boxplot(x='Churn', y='tenure', data=df)
plt.title("Tenure vs Churn")
plt.show()

# MonthlyCharges vs Churn
plt.figure(figsize=(6, 4))
sns.boxplot(x='Churn', y='MonthlyCharges', data=df)
plt.title("Monthly Charges vs Churn")
plt.show()

# Stage 4: Feature Engineering

# Create a new feature: average monthly spend rate
df['AvgSpendPerMonth'] = df['TotalCharges'] / (df['tenure'] + 1)

# Scale numeric columns
scaler = StandardScaler()
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'AvgSpendPerMonth']
df[num_cols] = scaler.fit_transform(df[num_cols])

df.head()

# Stage 5: Model Building

# Split into features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Logistic Regression model
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)
y_pred_log = log_model.predict(X_test)

# Random Forest model
rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Stage 6: Model Evaluation

def evaluate_model(name, y_true, y_pred, model):
    print(f"\n{name} Performance:")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("F1 Score:", classification_report(y_true, y_pred, zero_division=0))
    print("ROC-AUC:", roc_auc_score(y_true, model.predict_proba(X_test)[:,1]))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Evaluating both models
evaluate_model("Logistic Regression", y_test, y_pred_log, log_model)
evaluate_model("Random Forest", y_test, y_pred_rf, rf_model)

# Stage 7: Feature Importance

# Get feature importance from Random Forest model
importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Plot top 15 important features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importances.head(15))
plt.title("Top 15 Important Features for Churn Prediction")
plt.show()

# Observations
print("\nKey Insights:")
print("- Customers with shorter tenure and higher monthly charges are more likely to churn.")
print("- Contract type and payment method strongly influence churn.")
print("- Offering discounts or incentives to high-risk customers can help reduce churn.")

# ============================================
# Stage 7C: Improve Model Accuracy and Recall
# ============================================

# 1. Import required libraries
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report
)
from imblearn.over_sampling import SMOTE

# 2. Handle class imbalance using SMOTE
# This balances the churn vs. non-churn classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 3. Hyperparameter tuning for Random Forest using RandomizedSearchCV
param_grid = {
    'n_estimators': [200, 300, 400],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False],
    'max_features': ['sqrt', 'log2']
}

rf_model = RandomForestClassifier(random_state=42)
rf_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=8,
    cv=2,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='f1'
)

# 4. Train the optimized Random Forest model
rf_search.fit(X_resampled, y_resampled)
best_rf = rf_search.best_estimator_

# 5. Evaluate the tuned model
y_pred = best_rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, best_rf.predict_proba(X_test)[:, 1])

# 6. Display performance results
print("Improved Model Performance:")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1-score : {f1:.4f}")
print(f"ROC-AUC  : {roc_auc:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 7. Display top features influencing churn
feat_imp = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nTop Features Influencing Churn:")
print(feat_imp.head(10))

# ===============================
# Stage 8: Save, Explain and Export Results
# ===============================

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split

# Step 1: Ensure data exists
# If the DataFrame is not defined (e.g., after a runtime reset), re-load it
if 'df' not in locals():
    print("Dataframe 'df' not found. Please make sure you have loaded your dataset before running this stage.")
else:
    print("Data found. Proceeding...\n")

    # Separate features and target variable
    X = df.drop('Churn', axis=1)
    y = df['Churn']

    # Split data for testing
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Step 2: Retrain model if not already in memory
    if 'rf_model' not in locals():
        print("Model not found in memory. Retraining Random Forest model...")
        rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)
        rf_model.fit(X_train, y_train)
        print("Model retrained successfully.\n")
    else:
        # Check if the model is already fitted
        try:
            _ = rf_model.feature_importances_
        except Exception:
            print("Model not fitted. Retraining...")
            rf_model.fit(X_train, y_train)
        print("Model ready.\n")

    # Step 3: Evaluate model performance
    y_pred = rf_model.predict(X_test)
    y_prob = rf_model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)

    print("Model Performance Summary:")
    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-score : {f1:.4f}")
    print(f"ROC-AUC  : {roc_auc:.4f}\n")

    print("Detailed Classification Report:\n", classification_report(y_test, y_pred))

    # Step 4: Feature Importance
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf_model.feature_importances_
    }).sort_values(by='Importance', ascending=False)

    print("\nTop 10 Features Influencing Churn:")
    print(feature_importance.head(10))

    # Save feature importance to CSV
    feature_importance.to_csv('feature_importance_summary.csv', index=False)
    print("\nFeature importance saved as 'feature_importance_summary.csv'")

    # Step 5: Explain results in plain language
    top_features = feature_importance.head(5)['Feature'].tolist()
    print("\nPlain English Summary:")
    print(f"Customers are most likely to churn if they have issues or shorter tenure in these areas: {', '.join(top_features)}.")
    print("Improving customer satisfaction and retention in these areas can help reduce churn significantly.")

# =============================================================
# Stage 8: Final Evaluation & Insights
# =============================================================
# - Automatically handles missing models or data
# - Evaluates Random Forest model and prints human-friendly summary
# - Shows top features and saves them to CSV
# =============================================================

import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Helper: print section header
def section(title):
    print("\n" + "="*60)
    print(title)
    print("="*60 + "\n")

# -------------------------------------------------------------
# 1. Load or create model (rf_model)
# -------------------------------------------------------------
model_obtained = False

if 'rf_model' in globals():
    rf = rf_model
    print("Using rf_model already in memory.")
    model_obtained = True
elif os.path.exists("random_forest_churn_model.pkl"):
    rf = joblib.load("random_forest_churn_model.pkl")
    print("Loaded model from 'random_forest_churn_model.pkl'.")
    model_obtained = True
elif 'X_train' in globals() and 'y_train' in globals():
    print("Training a new RandomForest using X_train and y_train from memory...")
    rf = RandomForestClassifier(n_estimators=200, random_state=42)
    rf.fit(X_train, y_train)
    model_obtained = True
    print("Training complete.")
else:
    print("No model found. Please ensure rf_model or training data exists before running this stage.")
    raise SystemExit("Aborting Stage 8: No model available.")

# -------------------------------------------------------------
# 2. Ensure we have test data
# -------------------------------------------------------------
if 'X_test' not in globals() or 'y_test' not in globals():
    if 'X' in globals() and 'y' in globals():
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        print("Created X_test and y_test by splitting X and y from memory.")
    else:
        raise SystemExit("Missing X_test/y_test. Please run preprocessing and data split before this stage.")

# -------------------------------------------------------------
# 3. Evaluate model
# -------------------------------------------------------------
section("Model Evaluation")

y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)[:,1] if hasattr(rf, "predict_proba") else None

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)
roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None

print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1-score : {f1:.4f}")
if roc_auc is not None:
    print(f"ROC-AUC  : {roc_auc:.4f}")

# -------------------------------------------------------------
# 4. Feature Importance
# -------------------------------------------------------------
section("Feature Importance")

importances = pd.DataFrame({
    'Feature': X_test.columns,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=importances.head(15))
plt.title("Top 15 Important Features for Churn Prediction")
plt.tight_layout()
plt.show()

importances.to_csv("feature_importance_summary.csv", index=False)
print("Saved feature importance as 'feature_importance_summary.csv'.")

# -------------------------------------------------------------
# 5. Layman Summary (Final Report Style)
# -------------------------------------------------------------
section("Layman Summary")

print("The model predicts whether a customer will stop the service (churn) based on historical data.")
print(f"- The model we used is Random Forest and its accuracy on the test set is {accuracy*100:.2f}%.")
print("- The most important factors (features) the model used to decide are listed above.")
print("Examples in plain language:")
print(" • If MonthlyCharges is high and tenure is low, the customer is more likely to leave.")
print(" • Customers on month-to-month contracts are usually at higher risk than those on long-term contracts.")